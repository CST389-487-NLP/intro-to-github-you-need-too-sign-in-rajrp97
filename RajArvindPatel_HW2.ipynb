{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "RajArvindPatel_HW2.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CST389-487-NLP/intro-to-github-you-need-too-sign-in-rajrp97/blob/main/RajArvindPatel_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b38adc0c"
      },
      "source": [
        "# <font color=\"magenta\">Programming assignment 1</font>\n",
        "<font color=\"blue\"> 7pts - assignment is not optional, but counts as bonus</font>\n",
        "\n",
        "The assignment is structured as follows: the beginning cells are the starter code that you must read and understand. Then you'll see the red bold title \"Assignment\". The markdown cell that follows the title describes partial starter code in the next cell. This next cell has 'fill-in-blancks' parts that are your repsoncibility. Then the rest of the assignment is described in the following markdown cell\n",
        "\n",
        "\n",
        "## Training IMDB sentiment classification model\n",
        "Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf5edbda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1fdf48-103d-4b46-e687-1cc0cc1492b4"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install torchinfo\n",
        "!pip install torch==2.3.0 torchtext==0.18.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.21.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: typer>=0.23.1 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (0.23.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->huggingface-hub>=0.24.0->datasets) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->huggingface-hub>=0.24.0->datasets) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub>=0.24.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub>=0.24.0->datasets) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub>=0.24.0->datasets) (0.1.2)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: torchtext==0.18.0 in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.21.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.9.86)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary modules"
      ],
      "metadata": {
        "id": "aHHRAjOtKACa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "1pVG-C_EPLQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ccfcdf1-f3f5-4479-e1e7-950e41022dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7eb9fe350c70>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell inmports datasets library from Hugging face. To be able to run it you need to\n",
        "  - Create a Hugging Face account and log into it,\n",
        "  - Navigate to Settings: Click on your profile picture in the top-right corner, then select Settings from the dropdown menu,\n",
        "  - Go to Access Tokens: In the settings menu on the left side, click on the Access Tokens tab,\n",
        "  - Generate a New Token: Click the New Token button\n",
        "  - Configure the Token:\n",
        "    1.   Name your token\n",
        "    2.    Choose the access level. In this cases, a \"read\" token (read-only access to public and granted-access models/datasets) is sufficient and recommended for security (we do not need to push results onto Hugging face\n",
        "  - Click the Generate a token (or Create token) button. The full token value will be displayed immediately. Copy it, as it will not be shown in full again for security reasons\n",
        "Next you would need to store the token in your colab security locker\n",
        "  - Open Secrets: In your Google Colab notebook, click the key icon (ðŸ”‘) in the left-hand sidebar,\n",
        "  - Click the + Add new secret button,\n",
        "  - In the \"Name\" field, type `HF_TOKEN` (this is the standard name used by Hugging Face libraries but if you want to you can use your own),\n",
        "  - Paste your Hugging Face API token into the \"Value\" field\n",
        "  - Toggle the Notebook access switch to the right (it will turn blue) to allow your current notebook to use this secret\n",
        "For this notebook nothing else is needed - when you run code in the next cell you will be asked to allow hugging face access to notebook. Say 'yes' and it'll run. In the future you may need to use api to access Hugging face. In this case just use this code:\n",
        "\n",
        "```python\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Retrieve the secret\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "```"
      ],
      "metadata": {
        "id": "ctnBdWLuKXrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# 2. Access splits\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "# 3. View an example: {'text': 'I love sci-fi...', 'label': 1}\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "5KvGHOEbCcsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60c2272-16ed-4ac0-9f31-590343603443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57954931"
      },
      "source": [
        "##  Vocabulary Building\n",
        "Next cell imports two crucial functions from the torchtext library for natural language processing:\n",
        "  - `get_tokenizer`: This function is used to create a tokenizer. A tokenizer is responsible for breaking down raw text into smaller units called 'tokens' (usually words or subwords). For example, it can turn a sentence like 'Hello world!' into ['hello', 'world', '!'].\n",
        "  - `build_vocab_from_iterator`: This function constructs a vocabulary from an iterator that yields sequences of tokens. A vocabulary maps each unique token to a unique numerical index, which is essential for converting text into a numerical format that a neural network can process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "7lpCldoPRK6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell is we prepare the text data for a neural network by converting raw text into a numerical representation. It performs two main steps:\n",
        "- Define the tokenizer: The line tokenizer = `get_tokenizer(\"basic_english\", language=\"en\")` creates a tokenizer instance using torchtext's get_tokenizer function. This tokenizer is responsible for splitting raw sentences into individual words or tokens,\n",
        "- Build the vocabulary: This is the core part where a mapping from tokens to numerical indices is created.\n",
        "  - The 'yield_tokens' function is a helper generator. It iterates through the train_data (which contains dictionaries with a 'text' key) and applies the tokenizer to each text, yielding a stream of tokens. This stream is what `build_vocab_from_iterator` expects.\n",
        "  - `vocab = build_vocab_from_iterator(...)` constructs the vocabulary. It takes the token stream from `yield_tokens`, includes special tokens like <unk> (for unknown words not in the vocabulary) and <pad> (for padding sequences to the same length), and only includes words that appear at least `min_freq=2` times in the training data to reduce vocabulary size and noise.\n",
        "  - `vocab.set_default_index(vocab[\"<unk>\"])` ensures that any word encountered during inference that was not in the training vocabulary will be mapped to the index of the <unk> token.\n",
        "  - Finally, the print statements display information about the built vocabulary, such as its size and the indices of some specific words and special tokens, to verify its creation."
      ],
      "metadata": {
        "id": "jrgt_T8RPcbw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b849dcfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40356188-1415-429b-d8c0-3fd12dc9c26d"
      },
      "source": [
        "# 1. Define the tokenizer\n",
        "# A basic English tokenizer is sufficient for the IMDB dataset\n",
        "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
        "print(\"Tokenizer initialized.\")\n",
        "\n",
        "# 2. Build the vocabulary\n",
        "# Define a generator function to yield tokens from the training data\n",
        "def yield_tokens(data_iter):\n",
        "    # data_iter is a list of dictionaries like {'text': '...', 'label': 0}\n",
        "    for item in data_iter:\n",
        "        yield tokenizer(item['text'])\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(train_data), # Use the loaded train_data\n",
        "    specials=[\"<unk>\", \"<pad>\"], # Special tokens for unknown words and padding\n",
        "    min_freq=2 # Minimum frequency for a word to be included in the vocab\n",
        ")\n",
        "vocab.set_default_index(vocab[\"<unk>\"]) # Set default index for unknown words\n",
        "print(f\"Vocabulary built with {len(vocab)} unique tokens.\")\n",
        "\n",
        "# Display some vocabulary information for verification\n",
        "print(f\"<unk> token index: {vocab['<unk>']}\")\n",
        "print(f\"<pad> token index: {vocab['<pad>']}\")\n",
        "print(f\"Index of 'the': {vocab['the']}\")\n",
        "print(f\"Index of 'movie': {vocab['movie']}\")\n",
        "print(f\"Top 10 most frequent tokens: {vocab.get_itos()[:10]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer initialized.\n",
            "Vocabulary built with 51718 unique tokens.\n",
            "<unk> token index: 0\n",
            "<pad> token index: 1\n",
            "Index of 'the': 2\n",
            "Index of 'movie': 21\n",
            "Top 10 most frequent tokens: ['<unk>', '<pad>', 'the', '.', ',', 'and', 'a', 'of', 'to', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ba0722c"
      },
      "source": [
        "This cell defines a custom `IMDBDataset` class, which is a crucial component for handling your data in PyTorch, especially for tasks like natural language processing. We do the following:\n",
        "- import Dataset from `torch.utils.data`, which is the base class for custom datasets in PyTorch.\n",
        "- Define text_pipeline. It is the lambda function that takes a raw text string, tokenizes it using the tokenizer (defined in a previous cell), and then converts those tokens into numerical indices using the vocab (also built in a previous cell). This transforms text into a sequence of numbers that a neural network can process.\n",
        "- Define `label_pipeline`. This lambda function is designed to convert string labels (like 'pos' or 'neg') into numerical labels (1 or 0). For IMDB dataset, labels are already be integers, so this pipeline in not used here, but it a standard preprocessing step, so I kept it here.\n",
        "\n",
        "The IMDBDataset class inherits from torch `utils.data.Dataset` and has three main methods:\n",
        "  - `__init__` initializes the dataset with the raw data list (e.g., train_data), and the `text_pipeline` and `label_pipeline` functions,\n",
        "  - `__len__` returns the total number of items in the dataset, which is essential for PyTorch's DataLoader,\n",
        "  - `__getitem__` is the core method. When you access an item by index (e.g., dataset[0]), it retrieves the text and label for that index, applies the text_pipeline to convert text to token IDs, converts the label to a torch.long tensor (suitable for loss functions like CrossEntropyLoss), and then returns the processed label and text tensor,\n",
        "  - `Instantiate Datasets` code creates two instances of IMDBDataset:\n",
        "    - train_dataset_custom using train_data.\n",
        "    - test_dataset_custom using test_data.\n",
        "The rest are print statements to show the size of the created datasets and to display a sample item (label and first few token IDs) from the training dataset, ensuring that the data is being processed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d81ff29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90878841-30fe-4f06-dd03-a4dbdd4dd3c4"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 3. Define the data processing pipeline (text to numerical tensor) and custom Dataset\n",
        "# (Moved from previous cell to be part of the dataset class for better encapsulation)\n",
        "\n",
        "# Define the text pipeline using the previously built vocab and tokenizer\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "# Define the label pipeline to convert 'pos'/'neg' to 1/0\n",
        "# This pipeline is not strictly necessary here as labels from datasets.load_dataset are already integers.\n",
        "label_pipeline = lambda x: 1 if x == 'pos' else 0\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data_list, text_pipeline, label_pipeline):\n",
        "        self.data_list = data_list\n",
        "        self.text_pipeline = text_pipeline\n",
        "        self.label_pipeline = label_pipeline\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # self.data_list[idx] returns a dictionary like {'text': 'review content', 'label': 0}\n",
        "        # Direct unpacking into `text, label` fails for dictionaries. Instead, access by key.\n",
        "        item = self.data_list[idx]\n",
        "        text = item['text']\n",
        "        label = item['label'] # label is an integer (0 or 1)\n",
        "\n",
        "        processed_text = torch.tensor(self.text_pipeline(text), dtype=torch.int64)\n",
        "        # For CrossEntropyLoss, labels should be of type torch.long\n",
        "        processed_label = torch.tensor(label, dtype=torch.long)\n",
        "        return processed_label, processed_text\n",
        "\n",
        "# Create instances of the custom dataset\n",
        "print(\"Creating IMDB training dataset...\")\n",
        "train_dataset_custom = IMDBDataset(train_data, text_pipeline, label_pipeline)\n",
        "print(f\"Training dataset size: {len(train_dataset_custom)}\")\n",
        "\n",
        "print(\"Creating IMDB test dataset...\")\n",
        "test_dataset_custom = IMDBDataset(test_data, text_pipeline, label_pipeline)\n",
        "print(f\"Test dataset size: {len(test_dataset_custom)}\")\n",
        "\n",
        "# Verify a sample from the custom dataset\n",
        "print(\"\\nSample from custom training dataset (label, token_ids):\")\n",
        "sample_label, sample_text_tensor = train_dataset_custom[0]\n",
        "print(f\"Label: {sample_label}\")\n",
        "print(f\"Text tensor (first 10 tokens): {sample_text_tensor[:10]}\")\n",
        "print(f\"Original text (first 50 chars): {train_data[0]['text'][:50]}...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating IMDB training dataset...\n",
            "Training dataset size: 25000\n",
            "Creating IMDB test dataset...\n",
            "Test dataset size: 25000\n",
            "\n",
            "Sample from custom training dataset (label, token_ids):\n",
            "Label: 0\n",
            "Text tensor (first 10 tokens): tensor([   13,  1568,    13,   246, 35468,    43,    64,   398,  1135,    92])\n",
            "Original text (first 50 chars): I rented I AM CURIOUS-YELLOW from my video store b...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09c56e35"
      },
      "source": [
        "The next cell creates the efficient data loaders for nn PyTorch model, dealing with variable-length text sequences.\n",
        "- `Import DataLoader` part starts with importing DataLoader `from torch.utils.data`, which is PyTorch's utility for iterating over datasets in batches.\n",
        "- `Define collate_batch function` prepairs batches of data for the `torch.nn.EmbeddingBag` layer, I use in the nn model. When DataLoader yields a batch, it passes a list of samples (label, text_tensor) to this function. Here's what `collate_batch` does:\n",
        "  - It iterates through each sample in the batch.\n",
        "  - It collects all labels into label_list.\n",
        "  - It collects all processed text tensors (which are already numerical token IDs from the IMDBDataset) into text_list.\n",
        "  - It builds an offsets list. EmbeddingBag requires a single concatenated tensor of all text tokens in the batch, along with offsets that indicate where each individual text sequence starts in that concatenated tensor. So, offsets are cumulative sums of the lengths of the text sequences.`label_list` is stacked into a single tensor. Offsets are converted into a `torch.tensor` and `cumsum` (cumulative sum) is applied to correctly generate the starting indices for each text within the concatenated text_list. All `text_list` tensors are concatenated into one large text_list tensor.\n",
        "  - It returns the batch of labels, the concatenated text_list, and the offsets.\n",
        "\n",
        "In Create DataLoaders section I set up the actual data loaders for training and testing:\n",
        "- `BATCH_SIZE` is defined as 64, meaning 64 samples will be processed together.\n",
        "- `device` is set to 'cuda' if a GPU is available, otherwise 'cpu'.\n",
        "- `train_loader_custom` is created using `train_dataset_custom`, set to `shuffle=True` for mini-batch SGD, and importantly, uses `collate_batch` function.\n",
        "- `test_loader_custom` is created similarly for the test set, but `shuffle=False` as we only do forward pass for evaluation.\n",
        "\n",
        "Finally, we iterate through one batch from the train_loader_custom to print the shapes and a few samples of the labels, text tensor, and offsets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f59629e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10949452-0782-4148-a43f-3ac572f51a52"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 4. Define the collate_fn for DataLoader\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for _label, _text in batch:\n",
        "        label_list.append(_label)\n",
        "        processed_text = _text # _text is already a tensor from IMDBDataset\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0)) # Length of each text sequence\n",
        "\n",
        "    label_list = torch.stack(label_list) # Changed to torch.stack\n",
        "    # offsets.pop(0) is incorrect. cumsum should be applied to lengths before the first offset.\n",
        "    # The offsets for `torch.nn.EmbeddingBag` should be `[0, len(text1), len(text1)+len(text2), ...]`\n",
        "    # The first element is 0, and then each subsequent element is the cumulative sum of text lengths.\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "\n",
        "    # Concatenate all text tensors into a single tensor\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list, text_list, offsets\n",
        "\n",
        "# 5. Create DataLoaders\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Creating DataLoaders...\")\n",
        "train_loader_custom = DataLoader(\n",
        "    train_dataset_custom,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "test_loader_custom = DataLoader(\n",
        "    test_dataset_custom,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "print(\"DataLoaders created.\")\n",
        "\n",
        "# Verify by iterating through one batch\n",
        "print(\"\\nVerifying a batch from train_loader_custom:\")\n",
        "for labels, text_tensor, offsets in train_loader_custom:\n",
        "    print(f\"Batch labels shape: {labels.shape}\")\n",
        "    print(f\"Batch text tensor shape: {text_tensor.shape}\")\n",
        "    print(f\"Batch offsets shape: {offsets.shape}\")\n",
        "    print(f\"First few labels: {labels[:5]}\")\n",
        "    print(f\"First few text token IDs: {text_tensor[:10]}\")\n",
        "    print(f\"First few offsets: {offsets[:5]}\")\n",
        "    break\n",
        "\n",
        "print(\"\\nVerification of data loading pipeline complete. ModuleNotFoundError for torchdata.datapipes should be resolved with this custom pipeline.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating DataLoaders...\n",
            "DataLoaders created.\n",
            "\n",
            "Verifying a batch from train_loader_custom:\n",
            "Batch labels shape: torch.Size([64])\n",
            "Batch text tensor shape: torch.Size([18626])\n",
            "Batch offsets shape: torch.Size([64])\n",
            "First few labels: tensor([1, 0, 1, 1, 1])\n",
            "First few text token IDs: tensor([ 13, 260, 537,  38,   0,   5,   0, 143, 155,  14])\n",
            "First few offsets: tensor([   0,  461, 1057, 1292, 1437])\n",
            "\n",
            "Verification of data loading pipeline complete. ModuleNotFoundError for torchdata.datapipes should be resolved with this custom pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDlvgeuCajt6"
      },
      "source": [
        "### <font color=\"red\"><b>Assginment</b></font>\n",
        "<font color=\"blue\"> Assignment starts here and continues to the end of NB</font>\n",
        "\n",
        "The next cell must define the IMDBMLP (IMDB Multi-Layer Perceptron) model, which is a neural network designed for sentiment classification. Below I summarize the required structure and requirements.\n",
        " - `class IMDBMLP(nn.Module)` - class definition of the IMDBMLP class inherits from nn.Module, the base class for all neural network modules in PyTorch.\n",
        "- `__init__ method` is the constructor where all the layers of the network are defined:\n",
        "- `self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)` creates an embedding layer. `EmbeddingBag` is particularly efficient for text classification where you might have variable-length sequences. It takes the `vocab_size` (total number of unique words), `embedding_dim` (the size of the vector representation for each word), and `sparse=False` ensures dense gradients, which is generally better for optimizers like Adam.\n",
        "- `self.fc1 = ...` defines the first fully connected (linear) layer. It takes the output from the embedding layer (which is an `embedding_dim`-sized vector representing the entire text) and transforms it into hidden_dim1 features.\n",
        "- `self.gelu1 =...` is the hidden GELU (Gaussian Error Linear Unit) activation layer, which introduces non-linearity after the first linear layer.\n",
        "- `self.fc2 =...` is the second fully connected layer, further transforming features.\n",
        "- `self.gelu2 = nn.GELU()` another GELU activation layer.\n",
        "-`self.fc3 = ...` is the final linear layer, that creates logits, i.e.  outputs `num_class` values (2 in this case, for positive/negative sentiment).\n",
        "-`self.softmax = nn.Softmax(dim=1)` layer applies the Softmax function to the output of fc3 to convert the raw scores into probabilities for each class, ensuring they sum to 1.\n",
        "After defitinions of layers come methods:\n",
        "- `forward` method defines how data flows through the network during a forward pass:\n",
        "`embedded = self.embedding(text, offsets)`. The input text (token IDs) and offsets (indicating start positions of each text in a batch) are passed to the EmbeddingBag layer to get the text embeddings.\n",
        "- The embedded output then sequentially passes through fc1, gelu1, fc2, gelu2, and fc3. Finally, self.softmax(x) converts the raw logits into probability distribution over the classes.\n",
        "\n",
        "You must define the following model hyperparameters:\n",
        "-`VOCAB_SIZE` Determined by the `len(vocab)` from previous cells.\n",
        "-`EMBEDDING_DIM` is the dimensionality of the word embeddings (you can choose any value from 50 to 2024 but in your comments you must justify your choice).\n",
        "- `HIDDEN_DIM1, HIDDEN_DIM2` are the sizes (number of neurons) of the hidden layers in the MLP (again it is your choice but you should explain it).\n",
        "- `NUM_CLASSES` is 2 for binary sentiment classification.\n",
        "Next we instantiate the model `model_imdb_mlp = IMDBMLP(...)` - it creates an actual instance of your neural network. And then print model summary using `torchinfo.summary()` that provides a detailed overview of the model, including its layers, output shapes, number of parameters for each layer, and total parameters. The input_size and dtypes arguments are crucial here to simulate the input shape that the model expects for text (a 1D tensor of token IDs) and offsets (a 1D tensor of start indices) for a sample batch.\n",
        "\n",
        "<font color=\"red\">you must fill in the dots in the next section</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80e42719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788aa750-2471-4199-ce09-2d7c5735719b"
      },
      "source": [
        "import torch\n",
        "import torchinfo\n",
        "import torch.nn as nn\n",
        "\n",
        "class IMDBMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, num_class):\n",
        "        super(IMDBMLP, self).__init__()\n",
        "        # Set sparse=False to ensure dense gradients for compatibility with Adam optimizer\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim1)\n",
        "        self.gelu1 = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.gelu2 = nn.GELU()\n",
        "        self.fc3 = nn.Linear(hidden_dim2, num_class)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        x = self.fc1(embedded)\n",
        "        x = self.gelu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.gelu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Define model parameters\n",
        "VOCAB_SIZE = len(vocab) # vocab is from previous cells\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_DIM1 = 128\n",
        "HIDDEN_DIM2 = 64\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Instantiate the model\n",
        "model_imdb_mlp = IMDBMLP(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM1, HIDDEN_DIM2, NUM_CLASSES)\n",
        "\n",
        "# Print model summary\n",
        "print(\"IMDBMLP Model Summary:\")\n",
        "# Correct input_size: text should be 1D, offsets should be 1D\n",
        "# Example: one review with 10 tokens (total 10 tokens for batch size 1)\n",
        "torchinfo.summary(model_imdb_mlp, input_size=[(10,), (1,)], dtypes=[torch.int64, torch.int64])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDBMLP Model Summary:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "IMDBMLP                                  [1, 2]                    --\n",
              "â”œâ”€EmbeddingBag: 1-1                      [1, 64]                   3,309,952\n",
              "â”œâ”€Linear: 1-2                            [1, 128]                  8,320\n",
              "â”œâ”€GELU: 1-3                              [1, 128]                  --\n",
              "â”œâ”€Linear: 1-4                            [1, 64]                   8,256\n",
              "â”œâ”€GELU: 1-5                              [1, 64]                   --\n",
              "â”œâ”€Linear: 1-6                            [1, 2]                    130\n",
              "â”œâ”€Softmax: 1-7                           [1, 2]                    --\n",
              "==========================================================================================\n",
              "Total params: 3,326,658\n",
              "Trainable params: 3,326,658\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 3.33\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 13.31\n",
              "Estimated Total Size (MB): 13.31\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Plan**\n",
        "\n",
        "We use `nn.EmbeddingBag` to convert each variable-length IMDB review into a fixed-size vector of length `EMBEDDING_DIM`. The collate_batch function returns:\n",
        "\n",
        "**Text**: A 1D tensor containing all token IDs concatenated for the entire batch\n",
        "\n",
        "**Offsets:** A 1D tensor indicating where each review starts inside text\n",
        "\n",
        "**labels:** Sentiment labels (0 = negative, 1 = positive)\n",
        "\n",
        "**The model applies a 3-layer MLP:**\n",
        "* EmbeddingBag â†’ Linear â†’ GELU â†’ Linear â†’ GELU â†’ Linear â†’ Softmax.\n",
        "\n",
        "**Because the model returns probabilities using Softmax, we train using negative log-likelihood:**\n",
        "\n",
        "* We compute `log(probs)` and apply `nn.NLLLoss()`.\n",
        "\n",
        "**During training we report:**\n",
        "\n",
        "* Running loss (average loss over all samples seen in the epoch)\n",
        "\n",
        "* Running accuracy (correct predictions / total samples)\n",
        "\n",
        "* Finally, we evaluate on the test set and print:\n",
        "\n",
        "* Test loss\n",
        "\n",
        "* Test accuracy"
      ],
      "metadata": {
        "id": "nYTbFlbHud-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "# re-instantiate / move model to device (requirement)\n",
        "model_imdb_mlp = model_imdb_mlp.to(device)\n",
        "\n",
        "# correct loss for softmax output: NLLLoss with log-probabilities\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model_imdb_mlp.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for labels, text, offsets in loader:\n",
        "        labels = labels.to(device).long()\n",
        "        text = text.to(device)\n",
        "        offsets = offsets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        probs = model(text, offsets)                       # softmax probabilities\n",
        "        loss = criterion(torch.log(probs + 1e-12), labels) # log-probabilities\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        correct += (probs.argmax(dim=1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for labels, text, offsets in loader:\n",
        "            labels = labels.to(device).long()\n",
        "            text = text.to(device)\n",
        "            offsets = offsets.to(device)\n",
        "\n",
        "            probs = model(text, offsets)\n",
        "            loss = criterion(torch.log(probs + 1e-12), labels)\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            correct += (probs.argmax(dim=1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "# Training (prints running loss + accuracy per epoch)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = train_one_epoch(model_imdb_mlp, train_loader_custom)\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "# Test evaluation (prints test loss + accuracy)\n",
        "test_loss, test_acc = evaluate(model_imdb_mlp, test_loader_custom)\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW3W0WFzu4O9",
        "outputId": "cce1eba6-5b92-4f05-b078-bb59b5acd729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train Loss: 0.5043 | Train Acc: 0.7420\n",
            "Epoch 2/5 | Train Loss: 0.3048 | Train Acc: 0.8766\n",
            "Epoch 3/5 | Train Loss: 0.2207 | Train Acc: 0.9137\n",
            "Epoch 4/5 | Train Loss: 0.1625 | Train Acc: 0.9400\n",
            "Epoch 5/5 | Train Loss: 0.1187 | Train Acc: 0.9596\n",
            "Test Loss: 0.3951 | Test Acc: 0.8606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results Interpretation**\n",
        "\n",
        "* Training loss should generally decrease across epochs as the model learns.\n",
        "\n",
        "* Training accuracy should generally increase as predictions improve.\n",
        "\n",
        "* Test accuracy measures generalization on unseen reviews. If training accuracy is much higher than test accuracy, that suggests overfitting.\n",
        "\n",
        "* Because IMDB reviews are variable-length, EmbeddingBag is efficient since it pools token embeddings directly into a single vector without padding."
      ],
      "metadata": {
        "id": "dohD_xOYu9WD"
      }
    }
  ]
}